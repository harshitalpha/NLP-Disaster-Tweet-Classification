# -*- coding: utf-8 -*-
"""BERT ON DISASTER CLASSIFICATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NDV9DXifkXHJQ8sG6fCph3ryh04nObhp
"""

!pip install bert-for-tf2

import tensorflow_hub as hub
import pandas as pd
import seaborn as sns
import numpy as np
import tensorflow as tf
import re
import string
from tqdm import tqdm

import bert
from bert.tokenization.bert_tokenization import FullTokenizer
from bert import BertModelLayer
from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights
from sklearn.metrics import confusion_matrix, classification_report


import os

train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

def remove_url(text):
    return re.sub(r"https?:\/\/t.co\/[A-Za-z0-9]+","",text)
def remove_html(text):
    return re.sub(r"<.*?>","",text)
def remove_punctuation(text):
    table=str.maketrans('','',string.punctuation)
    return text.translate(table)

train_df['text'] = train_df['text'].apply(lambda x:remove_url(x))
test_df['text'] = test_df['text'].apply(lambda x:remove_url(x))
train_df['text'] = train_df['text'].apply(lambda x:remove_html(x))
test_df['text'] = test_df['text'].apply(lambda x:remove_html(x))
train_df['text'] = train_df['text'].apply(lambda x:remove_punctuation(x))
test_df['text'] = test_df['text'].apply(lambda x:remove_punctuation(x))
train_df['text'] = train_df['text'].apply(lambda x:x.lower())
test_df['text'] = test_df['text'].apply(lambda x:x.lower())



"""# Downloading Bert Model"""

!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip

!unzip uncased_L-12_H-768_A-12.zip

os.makedirs('model', exist_ok = True)

!mv uncased_L-12_H-768_A-12/ model

bert_model_name = "uncased_L-12_H-768_A-12"

bert_ckpt_dir = os.path.join('model',bert_model_name)
bert_ckpt_file = os.path.join(bert_ckpt_dir,'bert_model.ckpt')
bert_config_file = os.path.join(bert_ckpt_dir, 'bert_config.json')

"""# Pre Processing of the text
  * Tokenizing the text
  * Padding the seqence to max length
"""

class DisasterClassifier:
  DATA_COLUMN = "text"
  LABEL_COLUMN = "target"

  def __init__(self, train, test, max_seq_len, tokenizer:FullTokenizer):
    self.tokenizer = tokenizer
    self.max_seq_len = 0

    self.x_train, self.y_train = self._prepare_train(train)
    self.max_seq_len = min(self.max_seq_len, max_seq_len)
    self.x_train  = self._pad(self.x_train)
    self.x_test = self._prepare_test(test)
    self.x_test = self._pad(self.x_test)


  def _prepare_train(self, data):
    x, y = [] , []
    for _, row in tqdm(data.iterrows()):
      text, label =\
       row[DisasterClassifier.DATA_COLUMN], \
       row[DisasterClassifier.LABEL_COLUMN]
      
      tokens = self.tokenizer.tokenize(text)
      tokens = ["[CLS]"] + tokens + ["[SEP]"]
      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)

      self.max_seq_len = max(self.max_seq_len, len(token_ids))
      x.append(token_ids)
      y.append(label)
    return np.array(x), np.array(y)

  def _prepare_test(self, data):
    x = []
    for _, row in tqdm(data.iterrows()):
      text = row[DisasterClassifier.DATA_COLUMN]
      
      tokens = self.tokenizer.tokenize(text)
      tokens = ["[CLS]"] + tokens + ["[SEP]"]
      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
      x.append(token_ids)
    return np.array(x)
  
  def _pad(self, ids):
    x = []
    for input_ids in tqdm(ids):
      cut = min(len(input_ids), self.max_seq_len - 2)
      input_ids = input_ids[:cut]
      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))
      x.append(np.array(input_ids))      
    
    return np.array(x)

tokenizer = FullTokenizer(vocab_file= os.path.join(bert_ckpt_dir, "vocab.txt"))

data = DisasterClassifier(train_df, test_df, max_seq_len= 64, tokenizer=tokenizer)

def create_model(max_seq_len, bert_ckpt_dir, bert_config_file):
  with tf.io.gfile.GFile(bert_config_file, "r") as reader:
      bc = StockBertConfig.from_json_string(reader.read())
      bert_params = map_stock_config_to_params(bc)
      bert_params.adapter_size = None
      bert = BertModelLayer.from_params(bert_params, name="bert")

  input_ids = tf.keras.layers.Input(shape = (max_seq_len, ), dtype= tf.int32, name= 'input_ids')
  bert_output = bert(input_ids)

  bert_output = bert_output[:,0,:]

  drop_out = tf.keras.layers.Dropout(0.5)(bert_output)
  d_out = tf.keras.layers.Dense(768, activation='tanh')(drop_out)
  logits = tf.keras.layers.Dropout(0.5)(d_out)
  out = tf.keras.layers.Dense(2, activation='softmax')(logits)

  model = tf.keras.models.Model(inputs = input_ids, outputs = out)
  model.summary()

  load_stock_weights(bert, bert_ckpt_file)

  return model

model = create_model(data.max_seq_len, bert_ckpt_file, bert_config_file)

model.compile(
  optimizer=tf.keras.optimizers.Adam(1e-5),
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name="acc")]
)

data.x_train

data.y_train.shape



history = model.fit(
  x=data.x_train, 
  y=data.y_train,
  validation_split=0.1,
  batch_size=16,
  shuffle=True,
  epochs=5,
  verbose = 1
)

_, train_acc = model.evaluate(data.x_train, data.y_train)
print("train acc", train_acc)

y_tain_pred = model.predict(data.x_train).argmax(axis=-1)

y_pred = model.predict(data.x_test).argmax(axis=-1)

sample_sub=pd.read_csv('sample_submission.csv')
y_pre=np.round(y_pred).astype(int).reshape(3263)
sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})
sub.to_csv('submission.csv',index=False)

model.save("BERT_MODEL_1.h5")

print(classification_report(data.y_train, y_tain_pred))

from google.colab import drive
drive.mount('/content/gdrive')

!ls /content/gdrive/My\ Drive

model_save_name = 'BERT_Diaster_classificatoin.h5'
path = F"/content/gdrive/My Drive/{model_save_name}" 
model.save(path)

def create_model_2(max_seq_len, bert_ckpt_dir, bert_config_file):
  with tf.io.gfile.GFile(bert_config_file, "r") as reader:
      bc = StockBertConfig.from_json_string(reader.read())
      bert_params = map_stock_config_to_params(bc)
      bert_params.adapter_size = None
      bert = BertModelLayer.from_params(bert_params, name="bert")

  input_ids = tf.keras.layers.Input(shape = (max_seq_len, ), dtype= tf.int32, name= 'input_ids')
  bert_output = bert(input_ids)
  d_out = bert_output[:,0,:]
  d_out = tf.keras.layers.Dense(768, activation='relu',
                                activity_regularizer= tf.keras.regularizers.l2())(d_out)
  logits = tf.keras.layers.Dropout(0.3)(d_out)
  d_out = tf.keras.layers.Dense(256, activation='relu', 
                                activity_regularizer= tf.keras.regularizers.l2())(logits)
  logits = tf.keras.layers.Dropout(0.3)(d_out)
  out = tf.keras.layers.Dense(2, activation='softmax')(logits)

  model = tf.keras.models.Model(inputs = input_ids, outputs = out)
  model.summary()

  load_stock_weights(bert, bert_ckpt_file)

  return model

model_2 = create_model_2(data.max_seq_len, bert_ckpt_file, bert_config_file)

model_2.compile(
  optimizer=tf.keras.optimizers.Adam(1e-5),
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name="acc")]
)

history = model_2.fit(
  x=data.x_train, 
  y=data.y_train,
  validation_split=0.1,
  batch_size=32,
  shuffle=True,
  epochs= 5,
  verbose = 1
)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# y_pred = model_2.predict(data.x_test).argmax(axis=-1)

sample_sub=pd.read_csv('sample_submission.csv')
y_pre=np.round(y_pred).astype(int).reshape(3263)
sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})
sub.to_csv('submission2.csv',index=False)

sub.head(10)

"""* We can do some filtering on the basis of keyword as we see in the EDA notebook 
* First off all we retrive the probability
"""

y_pred = model.predict(data.x_test).argmax(axis=-1)

sample_sub=pd.read_csv('sample_submission.csv')
y_pre=np.round(y_pred).astype(int).reshape(3263)
sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})

train_df.fillna('None', inplace=True)

train_df.head()

ag = train_df.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})
ag.sort_values('Disaster Probability', ascending=False).head(10)

keyword_list = list(ag[(ag['Count']>2) & (ag['Disaster Probability']>=0.9)].index)
keyword_list

ids = test_df['id'][test_df.keyword.isin(keyword_list)].values
sub['target'][sub['id'].isin(ids)] = 1
sub.head()

sub_id = np.array(sub.target.values)

print(type(sub_id))

print(type(y_pred))

y_pred[:5]

count = 0
for i in range(len(sub_id)):
  if sub_id[i] != y_pred[i]:
    count = count + 1
print(count)

print(sub_id[0:25])
print(y_pred[0:25])

